{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/workstation/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/workstation/code/XAImethods/hf_cam_dev/Attack/art.ipynb 单元格 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/hf_cam_dev/Attack/art.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/hf_cam_dev/Attack/art.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mcuda(), labels\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/hf_cam_dev/Attack/art.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     adversarial \u001b[39m=\u001b[39m attack\u001b[39m.\u001b[39;49mgenerate(x\u001b[39m=\u001b[39;49minputs\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), y\u001b[39m=\u001b[39;49mlabels\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/hf_cam_dev/Attack/art.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     adversarial_samples\u001b[39m.\u001b[39mextend(adversarial)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/hf_cam_dev/Attack/art.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Save Adversarial Samples\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent.py:200\u001b[0m, in \u001b[0;36mProjectedGradientDescent.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39mGenerate adversarial samples and return them in an array.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m:return: An array holding the adversarial examples.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating adversarial samples.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attack\u001b[39m.\u001b[39;49mgenerate(x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:218\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mfor\u001b[39;00m rand_init_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mmax\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_random_init)):\n\u001b[1;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m rand_init_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    217\u001b[0m         \u001b[39m# first iteration: use the adversarial examples as they are the only ones we have now\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m         adv_x[batch_index_1:batch_index_2] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_batch(\n\u001b[1;32m    219\u001b[0m             x\u001b[39m=\u001b[39;49mbatch, targets\u001b[39m=\u001b[39;49mbatch_labels, mask\u001b[39m=\u001b[39;49mmask_batch, eps\u001b[39m=\u001b[39;49mbatch_eps, eps_step\u001b[39m=\u001b[39;49mbatch_eps_step\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m         adversarial_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_batch(\n\u001b[1;32m    223\u001b[0m             x\u001b[39m=\u001b[39mbatch, targets\u001b[39m=\u001b[39mbatch_labels, mask\u001b[39m=\u001b[39mmask_batch, eps\u001b[39m=\u001b[39mbatch_eps, eps_step\u001b[39m=\u001b[39mbatch_eps_step\n\u001b[1;32m    224\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:279\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._generate_batch\u001b[0;34m(self, x, targets, mask, eps, eps_step)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mfor\u001b[39;00m i_max_iter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i_max_iter \u001b[39m=\u001b[39m i_max_iter\n\u001b[0;32m--> 279\u001b[0m     adv_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_pytorch(\n\u001b[1;32m    280\u001b[0m         adv_x, inputs, targets, mask, eps, eps_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_random_init \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39mand\u001b[39;49;00m i_max_iter \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m, momentum\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[39mreturn\u001b[39;00m adv_x\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:435\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._compute_pytorch\u001b[0;34m(self, x, x_init, y, mask, eps, eps_step, random_init, momentum)\u001b[0m\n\u001b[1;32m    432\u001b[0m     x_adv \u001b[39m=\u001b[39m x\n\u001b[1;32m    434\u001b[0m \u001b[39m# Get perturbation\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m perturbation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_perturbation_pytorch(x_adv, y, mask, momentum)\n\u001b[1;32m    437\u001b[0m \u001b[39m# Apply perturbation and clip\u001b[39;00m\n\u001b[1;32m    438\u001b[0m x_adv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_perturbation_pytorch(x_adv, perturbation, eps_step)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:323\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._compute_perturbation_pytorch\u001b[0;34m(self, x, y, mask, momentum)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummary_writer\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    312\u001b[0m         batch_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_id,\n\u001b[1;32m    313\u001b[0m         global_step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i_max_iter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         targeted\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargeted,\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    322\u001b[0m \u001b[39m# Check for nan before normalisation an replace with 0\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39many(grad\u001b[39m.\u001b[39misnan()):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mElements of the loss gradient are NaN and have been replaced with 0.0.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m     grad[grad\u001b[39m.\u001b[39misnan()] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Dataset path\n",
    "data_path = \"/home/workstation/code/XAImethods/hf_cam_dev/ImageNet-Mini/images\"\n",
    "\n",
    "# Check if a file is a valid image file\n",
    "def is_valid_image_file(filename):\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    return any(filename.lower().endswith(ext) for ext in valid_extensions)\n",
    "\n",
    "# Check if a directory contains at least one valid image file\n",
    "def is_valid_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if is_valid_image_file(filename):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# List all directories\n",
    "all_directories = [d for d in os.listdir(data_path)]\n",
    "\n",
    "# List valid directories\n",
    "valid_directories = [d for d in all_directories if is_valid_directory(os.path.join(data_path, d))]\n",
    "\n",
    "# Backup and remove invalid directories temporarily\n",
    "backup_path = \"/tmp/invalid_dirs_backup\"\n",
    "os.makedirs(backup_path, exist_ok=True)\n",
    "for d in set(all_directories) - set(valid_directories):\n",
    "    shutil.move(os.path.join(data_path, d), backup_path)\n",
    "\n",
    "# Image preprocessing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Now load the dataset\n",
    "dataset = ImageFolder(root=data_path, transform=transform, is_valid_file=is_valid_image_file)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Restore the directories (optional based on whether you want to keep the structure intact)\n",
    "for d in os.listdir(backup_path):\n",
    "    shutil.move(os.path.join(backup_path, d), data_path)\n",
    "\n",
    "# Load the model\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Adversarial Attack\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0, 1),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 224, 224),\n",
    "    nb_classes=len(valid_directories)\n",
    ")\n",
    "attack = ProjectedGradientDescent(classifier, eps=0.03, max_iter=100, eps_step=0.01)\n",
    "\n",
    "adversarial_samples = []\n",
    "for inputs, labels in dataloader:\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    adversarial = attack.generate(x=inputs.cpu().numpy(), y=labels.cpu().numpy())\n",
    "    adversarial_samples.extend(adversarial)\n",
    "\n",
    "# Save Adversarial Samples\n",
    "def save_adversarial_samples(adversarial_samples, original_filepaths, save_directory):\n",
    "    for adv_sample, original_path in zip(adversarial_samples, original_filepaths):\n",
    "        sub_dir, filename = os.path.split(original_path)\n",
    "        _, sub_dir = os.path.split(sub_dir)\n",
    "        save_sub_dir = os.path.join(save_directory, sub_dir)\n",
    "        os.makedirs(save_sub_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_sub_dir, filename)\n",
    "        save_image(torch.tensor(adv_sample).permute(1, 2, 0), save_path)\n",
    "\n",
    "save_directory = \"/home/workstation/code/XAImethods/hf_cam_dev/attacked/ImageNet-Mini/images\"\n",
    "save_adversarial_samples(adversarial_samples, [sample[0] for sample in dataset.samples], save_directory)\n",
    "\n",
    "print(\"Adversarial sample generation and saving completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workstation/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/workstation/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 1 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 2 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 3 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 4 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 5 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 6 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 7 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 8 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 9 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 10 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 11 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 12 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 13 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 14 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 15 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 16 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 17 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 18 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 19 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 20 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 21 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 22 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 23 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 24 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 25 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 26 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 27 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 28 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 29 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 30 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 31 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 32 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 33 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 34 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 35 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 36 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 37 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 38 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 39 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 40 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 41 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 42 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 43 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 44 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 45 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 46 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 47 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 48 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 49 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 50 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 51 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 52 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 53 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 54 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 55 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 56 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 57 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 58 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 59 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 60 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 61 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 62 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 63 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 64 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 65 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 66 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 67 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 68 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 69 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 70 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 71 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 72 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 73 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 74 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 75 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 76 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 77 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 78 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 79 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 80 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 81 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 82 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 83 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 84 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 85 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 86 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 87 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 88 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 89 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 90 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 91 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 92 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 93 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 94 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 95 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 96 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 97 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 98 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 99 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 100 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 101 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 102 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 103 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 104 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 105 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 106 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 107 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 108 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 109 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 110 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 111 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 112 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 113 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 114 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 115 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 116 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 117 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 118 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 119 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 120 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 121 of 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 122 of 122\n",
      "Adversarial sample generation and saving completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "# Dataset path\n",
    "data_path = \"/home/workstation/code/XAImethods/hf_cam_dev/ImageNet-Mini/images\"\n",
    "\n",
    "# Check if a file is a valid image file\n",
    "def is_valid_image_file(filename):\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    return any(filename.lower().endswith(ext) for ext in valid_extensions)\n",
    "\n",
    "# Check if a directory contains at least one valid image file\n",
    "def is_valid_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if is_valid_image_file(filename):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# List all directories\n",
    "all_directories = [d for d in os.listdir(data_path)]\n",
    "\n",
    "# List valid directories\n",
    "valid_directories = [d for d in all_directories if is_valid_directory(os.path.join(data_path, d))]\n",
    "\n",
    "# Backup and remove invalid directories temporarily\n",
    "backup_path = \"/tmp/invalid_dirs_backup\"\n",
    "os.makedirs(backup_path, exist_ok=True)\n",
    "for d in set(all_directories) - set(valid_directories):\n",
    "    shutil.move(os.path.join(data_path, d), backup_path)\n",
    "\n",
    "# Image preprocessing transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Now load the dataset\n",
    "dataset = ImageFolder(root=data_path, transform=transform, is_valid_file=is_valid_image_file)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Restore the directories (optional based on whether you want to keep the structure intact)\n",
    "for d in os.listdir(backup_path):\n",
    "    shutil.move(os.path.join(backup_path, d), data_path)\n",
    "\n",
    "# Load the model\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Adversarial Attack\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0, 1),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 224, 224),\n",
    "    nb_classes=len(valid_directories)\n",
    ")\n",
    "attack = ProjectedGradientDescent(classifier, eps=0.03, max_iter=100, eps_step=0.01)\n",
    "# Save Adversarial Samples function\n",
    "def save_adversarial_samples(adversarial_samples, original_filepaths, save_directory):\n",
    "    for adv_sample, original_path in zip(adversarial_samples, original_filepaths):\n",
    "        sub_dir, filename = os.path.split(original_path)\n",
    "        _, sub_dir = os.path.split(sub_dir)\n",
    "        save_sub_dir = os.path.join(save_directory, sub_dir)\n",
    "        os.makedirs(save_sub_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_sub_dir, filename)\n",
    "        \n",
    "        # Convert to tensor if it's a numpy array\n",
    "        if isinstance(adv_sample, np.ndarray):\n",
    "            adv_sample = torch.from_numpy(adv_sample)\n",
    "        \n",
    "        # Rescale tensor values to [0, 1]\n",
    "        adv_tensor = (adv_sample - adv_sample.min()) / (adv_sample.max() - adv_sample.min())\n",
    "        \n",
    "        save_image(adv_tensor, save_path)\n",
    "\n",
    "\n",
    "save_directory = \"/home/workstation/code/XAImethods/hf_cam_dev/attacked/ImageNet-Mini/images\"\n",
    "\n",
    "# Generate and immediately save adversarial samples\n",
    "for i, (inputs, labels) in enumerate(dataloader):\n",
    "    if torch.cuda.is_available():\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    adversarial = attack.generate(x=inputs.cpu().numpy(), y=labels.cpu().numpy())\n",
    "    \n",
    "    # Ensure the adversarial samples are in the shape (batch_size, channels, height, width)\n",
    "    assert adversarial.shape == (len(inputs), 3, 224, 224), f\"Unexpected shape: {adversarial.shape}\"\n",
    "    \n",
    "    # Corrected slicing operation to handle potential batch size mismatch\n",
    "    start_idx = i * dataloader.batch_size\n",
    "    end_idx = start_idx + len(inputs)\n",
    "    save_adversarial_samples(adversarial, [sample[0] for sample in dataset.samples[start_idx:end_idx]], save_directory)\n",
    "    \n",
    "    print(f\"Processed and saved batch {i+1} of {len(dataloader)}\")\n",
    "\n",
    "print(\"Adversarial sample generation and saving completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
