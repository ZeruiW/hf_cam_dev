{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def load_images_from_directory(root_path: str):\n",
    "    \"\"\"\n",
    "    Load images from a directory with subfolders named after ImageNet labels.\n",
    "    Return a list of (image, label, filename) triples.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Iterate over each subfolder\n",
    "    for label in os.listdir(root_path):\n",
    "        label_path = os.path.join(root_path, label)\n",
    "        \n",
    "        # Check if it's indeed a folder\n",
    "        if os.path.isdir(label_path):\n",
    "            \n",
    "            # Iterate over each image in the subfolder\n",
    "            for image_file in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_file)\n",
    "                \n",
    "                # Check if it's an image file\n",
    "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img = Image.open(image_path)\n",
    "                    dataset.append((img, label, image_file))  # Add image filename here\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "current_dir = \"/home/workstation/code/XAImethods/hf_cam_dev\"\n",
    "\n",
    "dataset_path = f\"{current_dir}/ImageNet-Mini/images\"\n",
    "dataset = load_images_from_directory(dataset_path)\n",
    "\n",
    "\n",
    "with open(f\"{current_dir}/ImageNet-Mini/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "\n",
    "\n",
    "label_to_index_description = {v[0]: (k, v[1]) for k, v in imagenet_class_index.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from codecarbon import EmissionsTracker\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pytorch_grad_cam import run_dff_on_image\n",
    "from pytorch_grad_cam import (\n",
    "    GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n",
    "    AblationCAM, XGradCAM, EigenCAM, EigenGradCAM,\n",
    "    LayerCAM, FullGrad, GradCAMElementWise\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU!\")\n",
    "# dataset = load_dataset(\"huggingface/cats-image\")\n",
    "# image = dataset[\"test\"][\"image\"][0]\n",
    "# img_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "\"\"\" Model wrapper to return a tensor\"\"\"\n",
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "\"\"\" Translate the category name to the category index.\n",
    "    Some models aren't trained on Imagenet but on even larger datasets,\n",
    "    so we can't just assume that 761 will always be remote-control.\n",
    "\n",
    "\"\"\"\n",
    "def category_name_to_index(model, category_name):\n",
    "    name_to_index = dict((v, k) for k, v in model.config.id2label.items())\n",
    "    return name_to_index[category_name]\n",
    "    \n",
    "\"\"\" Helper function to run GradCAM on an image and create a visualization.\n",
    "    (note to myself: this is probably useful enough to move into the package)\n",
    "    If several targets are passed in targets_for_gradcam,\n",
    "    e.g different categories,\n",
    "    a visualization for each of them will be created.\n",
    "    \n",
    "\"\"\"\n",
    "# def run_grad_cam_on_image(model: torch.nn.Module,\n",
    "#                           target_layer: torch.nn.Module,\n",
    "#                           targets_for_gradcam: List[Callable],\n",
    "#                           reshape_transform: Optional[Callable],\n",
    "#                           input_tensor: torch.nn.Module=img_tensor,\n",
    "#                           input_image: Image=image,\n",
    "#                           method: Callable=GradCAM):\n",
    "#     with method(model=HuggingfaceToTensorModelWrapper(model),\n",
    "#                  target_layers=[target_layer],\n",
    "#                  reshape_transform=reshape_transform) as cam:\n",
    "\n",
    "#         # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
    "#         repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "\n",
    "#         batch_results = cam(input_tensor=repeated_tensor,\n",
    "#                             targets=targets_for_gradcam)\n",
    "#         results = []\n",
    "#         for grayscale_cam in batch_results:\n",
    "#             visualization = show_cam_on_image(np.float32(input_image)/255,\n",
    "#                                               grayscale_cam,\n",
    "#                                               use_rgb=True)\n",
    "#             # Make it weight less in the notebook:\n",
    "#             visualization = cv2.resize(visualization,\n",
    "#                                        (visualization.shape[1]//2, visualization.shape[0]//2))\n",
    "#             results.append(visualization)\n",
    "#         return np.hstack(results)\n",
    "    \n",
    "\n",
    "# Define the CAM algorithm you want to use\n",
    "# Options: GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, \n",
    "# EigenCAM, EigenGradCAM, LayerCAM, FullGrad, GradCAMElementWise\n",
    "CAM_ALGORITHM = GradCAM\n",
    "cam_algorithm_name = CAM_ALGORITHM.__name__\n",
    "    \n",
    "def run_grad_cam_on_image(model: torch.nn.Module,\n",
    "                          target_layer: torch.nn.Module,\n",
    "                          targets_for_gradcam: List[Callable],\n",
    "                          input_tensor: torch.nn.Module,\n",
    "                          input_image: Image,\n",
    "                          reshape_transform: Optional[Callable] = None,\n",
    "                          method: Callable = CAM_ALGORITHM):\n",
    "    with method(model=HuggingfaceToTensorModelWrapper(model),\n",
    "                target_layers=[target_layer],\n",
    "                reshape_transform=reshape_transform) as cam:\n",
    "\n",
    "        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
    "        repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "\n",
    "        batch_results = cam(input_tensor=repeated_tensor,\n",
    "                            targets=targets_for_gradcam)\n",
    "        results = []\n",
    "        grayscale_cams = []\n",
    "        for grayscale_cam in batch_results:\n",
    "            visualization = show_cam_on_image(np.float32(input_image) / 255,\n",
    "                                              grayscale_cam,\n",
    "                                              use_rgb=True)\n",
    "            # Make it weight less in the notebook:\n",
    "            visualization = cv2.resize(visualization,\n",
    "                                       (visualization.shape[1] // 2, visualization.shape[0] // 2))\n",
    "            results.append(visualization)\n",
    "            grayscale_cams.append(grayscale_cam)\n",
    "        return np.hstack(results), grayscale_cams\n",
    "    \n",
    "def print_top_categories(model, img_tensor, top_k=5):\n",
    "    logits = model(img_tensor.unsqueeze(0)).logits\n",
    "    indices = logits.cpu()[0, :].detach().numpy().argsort()[-top_k :][::-1]\n",
    "    for i in indices:\n",
    "        print(f\"Predicted class {i}: {model.config.id2label[i]}\")\n",
    "\n",
    "# Generate targets_for_gradcam based on model's predictions\n",
    "def get_top_k_targets(model, input_tensor, k=5):\n",
    "    logits = model(input_tensor.unsqueeze(0)).logits\n",
    "    top_k_indices = logits[0].argsort(descending=True)[:k].cpu().numpy()\n",
    "    return [ClassifierOutputTarget(index) for index in top_k_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:59:27] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:59:27] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:59:27] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:59:27] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:59:27] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:59:29] CPU Model on constant consumption mode: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
      "[codecarbon INFO @ 12:59:29] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:59:29]   Platform system: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 12:59:29]   Python version: 3.10.9\n",
      "[codecarbon INFO @ 12:59:29]   Available RAM : 15.576 GB\n",
      "[codecarbon INFO @ 12:59:29]   CPU count: 16\n",
      "[codecarbon INFO @ 12:59:29]   CPU model: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
      "[codecarbon INFO @ 12:59:29]   GPU count: 1\n",
      "[codecarbon INFO @ 12:59:29]   GPU model: 1 x NVIDIA GeForce RTX 4090\n",
      "  3%|▎         | 1/39 [00:11<07:07, 11.25s/it][codecarbon INFO @ 12:59:47] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:59:47] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:59:47] Energy consumed for all CPUs : 0.000198 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:59:47] 0.000222 kWh of electricity used since the begining.\n",
      "  5%|▌         | 2/39 [00:26<08:03, 13.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb 单元格 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m dynamic_targets_for_gradcam \u001b[39m=\u001b[39m [ClassifierOutputTarget(index)]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# print(\"Input tensor shape:\", img_tensor.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# print(\"Calculated width:\", img_tensor.shape[2]//32)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# print(\"Calculated height:\", img_tensor.shape[1]//32)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m gradcam_result, grayscale_cams \u001b[39m=\u001b[39m run_grad_cam_on_image(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     target_layer\u001b[39m=\u001b[39;49mtarget_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     targets_for_gradcam\u001b[39m=\u001b[39;49mdynamic_targets_for_gradcam,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     input_tensor\u001b[39m=\u001b[39;49mimg_tensor,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     input_image\u001b[39m=\u001b[39;49mimg,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     reshape_transform\u001b[39m=\u001b[39;49mpartial(reshape_gradcam_transform_convnext_huggingface, model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m logits \u001b[39m=\u001b[39m model(img_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mlogits\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m top_indices \u001b[39m=\u001b[39m logits[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margsort(descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:\u001b[39m5\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb 单元格 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mwith\u001b[39;00m method(model\u001b[39m=\u001b[39mHuggingfaceToTensorModelWrapper(model),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m             target_layers\u001b[39m=\u001b[39m[target_layer],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m             reshape_transform\u001b[39m=\u001b[39mreshape_transform) \u001b[39mas\u001b[39;00m cam:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39m# Replicate the tensor for each of the categories we want to create Grad-CAM for:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     repeated_tensor \u001b[39m=\u001b[39m input_tensor[\u001b[39mNone\u001b[39;00m, :]\u001b[39m.\u001b[39mrepeat(\u001b[39mlen\u001b[39m(targets_for_gradcam), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     batch_results \u001b[39m=\u001b[39m cam(input_tensor\u001b[39m=\u001b[39;49mrepeated_tensor,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m                         targets\u001b[39m=\u001b[39;49mtargets_for_gradcam)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     grayscale_cams \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[1;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_input_gradient:\n\u001b[1;32m     71\u001b[0m     input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable(input_tensor,\n\u001b[1;32m     72\u001b[0m                                            requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivations_and_grads(input_tensor)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     target_categories \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/pytorch_grad_cam/activations_and_gradients.py:42\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients \u001b[39m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb 单元格 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/facebook-convnext-tiny-224.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:436\u001b[0m, in \u001b[0;36mConvNextForImageClassification.forward\u001b[0;34m(self, pixel_values, labels, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 436\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvnext(pixel_values, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states, return_dict\u001b[39m=\u001b[39;49mreturn_dict)\n\u001b[1;32m    438\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mpooler_output \u001b[39mif\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    440\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:371\u001b[0m, in \u001b[0;36mConvNextModel.forward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    369\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m--> 371\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    372\u001b[0m     embedding_output,\n\u001b[1;32m    373\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    374\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    377\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    379\u001b[0m \u001b[39m# global average pooling, (N, C, H, W) -> (N, C)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:262\u001b[0m, in \u001b[0;36mConvNextEncoder.forward\u001b[0;34m(self, hidden_states, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    260\u001b[0m         all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 262\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_module(hidden_states)\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    265\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:225\u001b[0m, in \u001b[0;36mConvNextStage.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mFloatTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    224\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsampling_layer(hidden_states)\n\u001b[0;32m--> 225\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(hidden_states)\n\u001b[1;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/convnext/modeling_convnext.py:187\u001b[0m, in \u001b[0;36mConvNextLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    185\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(x)\n\u001b[1;32m    186\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpwconv1(x)\n\u001b[0;32m--> 187\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(x)\n\u001b[1;32m    188\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpwconv2(x)\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_scale_parameter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:00:02] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 13:00:02] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 13:00:02] Energy consumed for all CPUs : 0.000396 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 13:00:02] 0.000445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 13:00:17] Energy consumed for RAM : 0.000073 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 13:00:17] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 13:00:17] Energy consumed for all CPUs : 0.000594 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 13:00:17] 0.000667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 13:00:32] Energy consumed for RAM : 0.000097 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 13:00:32] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 13:00:32] Energy consumed for all CPUs : 0.000792 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 13:00:32] 0.000889 kWh of electricity used since the begining.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "###################\n",
    "from transformers import ConvNextForImageClassification\n",
    "import torch\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "###################\n",
    "\n",
    "def reshape_transform_convnext_huggingface(tensor, model):\n",
    "    batch, features, height, width = tensor.shape\n",
    "    tensor = tensor.transpose(1, 2).transpose(2, 3)\n",
    "    norm = model.convnext.layernorm(tensor)\n",
    "    return norm.transpose(2, 3).transpose(1, 2)\n",
    "\n",
    "def reshape_gradcam_transform_convnext_huggingface(tensor, model):\n",
    "    batch, features, height, width = tensor.shape\n",
    "    tensor = tensor.transpose(1, 2).transpose(2, 3)\n",
    "    return tensor.transpose(2, 3).transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"facebook/convnext-tiny-224\"\n",
    "\n",
    "# Initialize tracking variables\n",
    "start_time = datetime.datetime.now()\n",
    "emissions_reports = []\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "\n",
    "def ensure_rgb(img):\n",
    "    if img.mode != 'RGB':\n",
    "        return img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "def is_valid_image_file(filepath):\n",
    "    \"\"\"Check if the file is a valid image file.\"\"\"\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            img.verify()  # verify that it is a valid image\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "num_batches = len(dataset) // BATCH_SIZE + (1 if len(dataset) % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "save_dir = f\"{current_dir}/results/{model_name}/{cam_algorithm_name}\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "for batch_num in tqdm(range(num_batches)):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min((batch_num + 1) * BATCH_SIZE, len(dataset))\n",
    "\n",
    "    model = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\").to(device)\n",
    "    target_layer = model.convnext.encoder.stages[-1].layers[-1]\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        img, label, filename = dataset[idx]\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            img = ensure_rgb(img)\n",
    "            # resize_transform = transforms.Resize((480, 640))\n",
    "            # img = resize_transform(img)\n",
    "            img_tensor = transform(img).to(device)\n",
    "            #print(img_tensor.shape)\n",
    "\n",
    "\n",
    "            index_description = label_to_index_description.get(label)\n",
    "            if index_description is None:\n",
    "                print(f\"Warning: Label '{label}' not found in the JSON file!\")\n",
    "                continue\n",
    "\n",
    "            index_str, description = index_description\n",
    "            index = int(index_str)\n",
    "            dynamic_targets_for_gradcam = [ClassifierOutputTarget(index)]\n",
    "\n",
    "            \n",
    "\n",
    "            # print(\"Input tensor shape:\", img_tensor.shape)\n",
    "            # print(\"Calculated width:\", img_tensor.shape[2]//32)\n",
    "            # print(\"Calculated height:\", img_tensor.shape[1]//32)\n",
    "\n",
    "\n",
    "            gradcam_result, grayscale_cams = run_grad_cam_on_image(\n",
    "                model=model,\n",
    "                target_layer=target_layer,\n",
    "                targets_for_gradcam=dynamic_targets_for_gradcam,\n",
    "                input_tensor=img_tensor,\n",
    "                input_image=img,\n",
    "                reshape_transform=partial(reshape_gradcam_transform_convnext_huggingface, model=model)\n",
    "            )\n",
    "\n",
    "            logits = model(img_tensor.unsqueeze(0)).logits\n",
    "            top_indices = logits[0].argsort(descending=True)[:5].cpu().numpy()\n",
    "            predictions = {index: {\"score\": logits[0][index].item(), \"label\": model.config.id2label[index]} for index in top_indices}\n",
    "            \n",
    "            img_dir = os.path.join(save_dir, filename.rsplit('.', 1)[0])\n",
    "            if not os.path.exists(img_dir):\n",
    "                os.makedirs(img_dir)\n",
    "\n",
    "            img_name = os.path.join(img_dir, \"original.jpg\")\n",
    "            gradcam_name = os.path.join(img_dir, \"gradcam.jpg\")\n",
    "            grayscale_name = os.path.join(img_dir, \"grayscale.jpg\")\n",
    "            grayscale_npy_name = os.path.join(img_dir, \"grayscale.npy\")\n",
    "            scores_name = os.path.join(img_dir, \"scores.npy\")\n",
    "            info_name = os.path.join(img_dir, \"info.txt\")\n",
    "\n",
    "            img.save(img_name)\n",
    "            Image.fromarray(gradcam_result).save(gradcam_name)\n",
    "            Image.fromarray((grayscale_cams[0] * 255).astype(np.uint8)).save(grayscale_name)\n",
    "            np.save(grayscale_npy_name, grayscale_cams[0])\n",
    "\n",
    "            scores = [data[\"score\"] for _, data in predictions.items()]\n",
    "            np.save(scores_name, scores)\n",
    "\n",
    "            with open(info_name, 'w') as f:\n",
    "                for index, data in predictions.items():\n",
    "                    label = data[\"label\"]\n",
    "                    score = data[\"score\"]\n",
    "                    f.write(f\"Class {index} ({label}): {score:.2f}\\n\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"CUDA OutOfMemoryError encountered for file: {filename}\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# After processing\n",
    "tracker.stop()\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Get emissions data\n",
    "emissions_data = tracker.get_emissions()\n",
    "\n",
    "# Save emissions report\n",
    "report_filename = os.path.join(save_dir, \"emissions_report.txt\")\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(emissions_data, f, indent=4)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Processing started at: {start_time}\")\n",
    "print(f\"Processing ended at: {end_time}\")\n",
    "print(f\"Duration: {end_time - start_time}\")\n",
    "print(f\"Emissions report saved to: {report_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
