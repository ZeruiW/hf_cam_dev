{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def load_images_from_directory(root_path: str):\n",
    "    \"\"\"\n",
    "    Load images from a directory with subfolders named after ImageNet labels.\n",
    "    Return a list of (image, label, filename) triples.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Iterate over each subfolder\n",
    "    for label in os.listdir(root_path):\n",
    "        label_path = os.path.join(root_path, label)\n",
    "        \n",
    "        # Check if it's indeed a folder\n",
    "        if os.path.isdir(label_path):\n",
    "            \n",
    "            # Iterate over each image in the subfolder\n",
    "            for image_file in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_file)\n",
    "                \n",
    "                # Check if it's an image file\n",
    "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img = Image.open(image_path)\n",
    "                    dataset.append((img, label, image_file))  # Add image filename here\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "current_dir = \"/home/workstation/code/XAImethods/hf_cam_dev\"\n",
    "\n",
    "dataset_path = f\"{current_dir}/ImageNet-Mini/images\"\n",
    "dataset = load_images_from_directory(dataset_path)\n",
    "\n",
    "\n",
    "with open(f\"{current_dir}/ImageNet-Mini/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "\n",
    "\n",
    "label_to_index_description = {v[0]: (k, v[1]) for k, v in imagenet_class_index.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from codecarbon import EmissionsTracker\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pytorch_grad_cam import run_dff_on_image\n",
    "from pytorch_grad_cam import (\n",
    "    GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n",
    "    AblationCAM, XGradCAM, EigenCAM, EigenGradCAM,\n",
    "    LayerCAM, FullGrad, GradCAMElementWise\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU!\")\n",
    "# dataset = load_dataset(\"huggingface/cats-image\")\n",
    "# image = dataset[\"test\"][\"image\"][0]\n",
    "# img_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "\"\"\" Model wrapper to return a tensor\"\"\"\n",
    "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "\"\"\" Translate the category name to the category index.\n",
    "    Some models aren't trained on Imagenet but on even larger datasets,\n",
    "    so we can't just assume that 761 will always be remote-control.\n",
    "\n",
    "\"\"\"\n",
    "def category_name_to_index(model, category_name):\n",
    "    name_to_index = dict((v, k) for k, v in model.config.id2label.items())\n",
    "    return name_to_index[category_name]\n",
    "    \n",
    "\"\"\" Helper function to run GradCAM on an image and create a visualization.\n",
    "    (note to myself: this is probably useful enough to move into the package)\n",
    "    If several targets are passed in targets_for_gradcam,\n",
    "    e.g different categories,\n",
    "    a visualization for each of them will be created.\n",
    "    \n",
    "\"\"\"\n",
    "# def run_grad_cam_on_image(model: torch.nn.Module,\n",
    "#                           target_layer: torch.nn.Module,\n",
    "#                           targets_for_gradcam: List[Callable],\n",
    "#                           reshape_transform: Optional[Callable],\n",
    "#                           input_tensor: torch.nn.Module=img_tensor,\n",
    "#                           input_image: Image=image,\n",
    "#                           method: Callable=GradCAM):\n",
    "#     with method(model=HuggingfaceToTensorModelWrapper(model),\n",
    "#                  target_layers=[target_layer],\n",
    "#                  reshape_transform=reshape_transform) as cam:\n",
    "\n",
    "#         # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
    "#         repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "\n",
    "#         batch_results = cam(input_tensor=repeated_tensor,\n",
    "#                             targets=targets_for_gradcam)\n",
    "#         results = []\n",
    "#         for grayscale_cam in batch_results:\n",
    "#             visualization = show_cam_on_image(np.float32(input_image)/255,\n",
    "#                                               grayscale_cam,\n",
    "#                                               use_rgb=True)\n",
    "#             # Make it weight less in the notebook:\n",
    "#             visualization = cv2.resize(visualization,\n",
    "#                                        (visualization.shape[1]//2, visualization.shape[0]//2))\n",
    "#             results.append(visualization)\n",
    "#         return np.hstack(results)\n",
    "    \n",
    "\n",
    "# Define the CAM algorithm you want to use\n",
    "# Options: GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, \n",
    "# EigenCAM, EigenGradCAM, LayerCAM, FullGrad, GradCAMElementWise\n",
    "CAM_ALGORITHM = GradCAM\n",
    "cam_algorithm_name = CAM_ALGORITHM.__name__\n",
    "    \n",
    "def run_grad_cam_on_image(model: torch.nn.Module,\n",
    "                          target_layer: torch.nn.Module,\n",
    "                          targets_for_gradcam: List[Callable],\n",
    "                          input_tensor: torch.nn.Module,\n",
    "                          input_image: Image,\n",
    "                          reshape_transform: Optional[Callable] = None,\n",
    "                          method: Callable = CAM_ALGORITHM):\n",
    "    with method(model=HuggingfaceToTensorModelWrapper(model),\n",
    "                target_layers=[target_layer],\n",
    "                reshape_transform=reshape_transform) as cam:\n",
    "\n",
    "        # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
    "        repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "\n",
    "        batch_results = cam(input_tensor=repeated_tensor,\n",
    "                            targets=targets_for_gradcam)\n",
    "        results = []\n",
    "        grayscale_cams = []\n",
    "        for grayscale_cam in batch_results:\n",
    "            visualization = show_cam_on_image(np.float32(input_image) / 255,\n",
    "                                              grayscale_cam,\n",
    "                                              use_rgb=True)\n",
    "            # Make it weight less in the notebook:\n",
    "            visualization = cv2.resize(visualization,\n",
    "                                       (visualization.shape[1] // 2, visualization.shape[0] // 2))\n",
    "            results.append(visualization)\n",
    "            grayscale_cams.append(grayscale_cam)\n",
    "        return np.hstack(results), grayscale_cams\n",
    "    \n",
    "def print_top_categories(model, img_tensor, top_k=5):\n",
    "    logits = model(img_tensor.unsqueeze(0)).logits\n",
    "    indices = logits.cpu()[0, :].detach().numpy().argsort()[-top_k :][::-1]\n",
    "    for i in indices:\n",
    "        print(f\"Predicted class {i}: {model.config.id2label[i]}\")\n",
    "\n",
    "# Generate targets_for_gradcam based on model's predictions\n",
    "def get_top_k_targets(model, input_tensor, k=5):\n",
    "    logits = model(input_tensor.unsqueeze(0)).logits\n",
    "    top_k_indices = logits[0].argsort(descending=True)[:k].cpu().numpy()\n",
    "    return [ClassifierOutputTarget(index) for index in top_k_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apple/mobilevit-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from collections import defaultdict\n",
    "# import gc\n",
    "\n",
    "# # Import the necessary modules\n",
    "# from transformers import MobileViTForImageClassification\n",
    "\n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "\n",
    "# # Load the model and send it to the appropriate device\n",
    "# model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\").to(device)\n",
    "# target_layer = model.mobilevit.conv_1x1_exp\n",
    "\n",
    "# # Define the transformation to convert images to tensor\n",
    "# transform = transforms.ToTensor()\n",
    "\n",
    "# # Define the directory to save results\n",
    "# save_dir = \"/home/workstation/code/XAImethods/pytorch-grad-cam/apple/mobilevit-small\"\n",
    "# # Ensure the directory exists\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# # Convert any grayscale image to RGB\n",
    "# def ensure_rgb(img):\n",
    "#     if img.mode != 'RGB':\n",
    "#         return img.convert('RGB')\n",
    "#     return img\n",
    "\n",
    "# # Iterate over all images in the dataset\n",
    "# for idx, (img, label, filename) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     img = ensure_rgb(img)  # Ensure the image is in RGB format\n",
    "#     img_tensor = transform(img).to(device)\n",
    "#     # Dynamically generate targets_for_gradcam based on model's prediction\n",
    "#     #dynamic_targets_for_gradcam = get_top_k_targets(model, img_tensor)\n",
    "\n",
    "#     # 使用标签获取索引和描述\n",
    "#     index_description = label_to_index_description.get(label)\n",
    "#     if index_description is None:\n",
    "#         print(f\"Warning: Label '{label}' not found in the JSON file!\")\n",
    "#         continue\n",
    "    \n",
    "#     index_str, description = index_description\n",
    "#     index = int(index_str)  # 将字符串索引转换为整数\n",
    "\n",
    "#     dynamic_targets_for_gradcam = [ClassifierOutputTarget(index)]\n",
    "\n",
    "#     gradcam_result, grayscale_cams = run_grad_cam_on_image(\n",
    "#         model=model,\n",
    "#         target_layer=target_layer,\n",
    "#         targets_for_gradcam=dynamic_targets_for_gradcam,\n",
    "#         input_tensor=img_tensor,\n",
    "#         input_image=img,\n",
    "#         reshape_transform=None\n",
    "#     )\n",
    "\n",
    "#     # Extract top predictions\n",
    "#     logits = model(img_tensor.unsqueeze(0)).logits\n",
    "#     top_indices = logits[0].argsort(descending=True)[:5].cpu().numpy()\n",
    "#     predictions = {index: {\"score\": logits[0][index].item(), \"label\": model.config.id2label[index]} for index in top_indices}\n",
    "    \n",
    "#     # Define individual directory for this image using the original filename\n",
    "#     img_dir = os.path.join(save_dir, filename.rsplit('.', 1)[0])  # Remove file extension from filename\n",
    "#     if not os.path.exists(img_dir):\n",
    "#         os.makedirs(img_dir)\n",
    "    \n",
    "#     # Define file names\n",
    "#     img_name = os.path.join(img_dir, \"original.jpg\")\n",
    "#     gradcam_name = os.path.join(img_dir, \"gradcam.jpg\")\n",
    "#     grayscale_name = os.path.join(img_dir, \"grayscale.jpg\")\n",
    "#     grayscale_npy_name = os.path.join(img_dir, \"grayscale.npy\")\n",
    "#     scores_name = os.path.join(img_dir, \"scores.npy\")\n",
    "#     info_name = os.path.join(img_dir, \"info.txt\")\n",
    "\n",
    "#     # Save the images and results\n",
    "#     img.save(img_name)\n",
    "#     Image.fromarray(gradcam_result).save(gradcam_name)\n",
    "#     Image.fromarray((grayscale_cams[0] * 255).astype(np.uint8)).save(grayscale_name)\n",
    "#     np.save(grayscale_npy_name, grayscale_cams[0])\n",
    "\n",
    "#     # Save the scores\n",
    "#     scores = [data[\"score\"] for _, data in predictions.items()]\n",
    "#     np.save(scores_name, scores)\n",
    "\n",
    "#     # Save the other info\n",
    "#     with open(info_name, 'w') as f:\n",
    "#         for index, data in predictions.items():\n",
    "#             label = data[\"label\"]\n",
    "#             score = data[\"score\"]\n",
    "#             f.write(f\"Class {index} ({label}): {score:.2f}\\n\")\n",
    "\n",
    "#     del img, label, img_name, img_tensor, gradcam_result, grayscale_cams, logits, top_indices, predictions, scores, index_description, index_str, description, index, dynamic_targets_for_gradcam\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# tracker.stop()\n",
    "# emissions_data = tracker.get_emissions()\n",
    "\n",
    "# output_dir = \"/home/workstation/code/XAImethods/pytorch-grad-cam/apple\"\n",
    "# tracker = EmissionsTracker(output_dir=output_dir)\n",
    "\n",
    "# print(emissions_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from collections import defaultdict\n",
    "# import gc\n",
    "# from transformers import MobileViTForImageClassification\n",
    "# import os\n",
    "# from PIL import Image\n",
    "\n",
    "# MAX_FILE_SIZE = 5 * 1024 * 1024  # e.g., 5 MB\n",
    "\n",
    "# def is_valid_image_file(filepath):\n",
    "#     \"\"\"Check if the file is a valid image file.\"\"\"\n",
    "#     try:\n",
    "#         with Image.open(filepath) as img:\n",
    "#             img.verify()  # verify that it is a valid image\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "    \n",
    "# tracker = EmissionsTracker()\n",
    "# tracker.start()\n",
    "\n",
    "# BATCH_SIZE = 100\n",
    "\n",
    "# # Split the dataset into batches\n",
    "# num_batches = len(dataset) // BATCH_SIZE + (1 if len(dataset) % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "# for batch_num in range(num_batches):\n",
    "#     start_idx = batch_num * BATCH_SIZE\n",
    "#     end_idx = min((batch_num + 1) * BATCH_SIZE, len(dataset))\n",
    "\n",
    "#     for idx in range(start_idx, end_idx):\n",
    "#         data = dataset[idx]\n",
    "\n",
    "#         try:\n",
    "\n",
    "\n",
    "#             # Load the model and send it to the appropriate device\n",
    "#             model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\").to(device)\n",
    "#             target_layer = model.mobilevit.conv_1x1_exp\n",
    "\n",
    "#             # Define the transformation to convert images to tensor\n",
    "#             transform = transforms.ToTensor()\n",
    "\n",
    "#             # Define the directory to save results\n",
    "#             save_dir = \"/home/workstation/code/XAImethods/pytorch-grad-cam/apple/mobilevit-small\"\n",
    "#             # Ensure the directory exists\n",
    "#             if not os.path.exists(save_dir):\n",
    "#                 os.makedirs(save_dir)\n",
    "#             # Convert any grayscale image to RGB\n",
    "#             def ensure_rgb(img):\n",
    "#                 if img.mode != 'RGB':\n",
    "#                     return img.convert('RGB')\n",
    "#                 return img\n",
    "\n",
    "#             # Iterate over all images in the dataset\n",
    "#             for idx, (img, label, filename) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 img = ensure_rgb(img)  # Ensure the image is in RGB format\n",
    "#                 img_tensor = transform(img).to(device)\n",
    "#                 # Dynamically generate targets_for_gradcam based on model's prediction\n",
    "#                 #dynamic_targets_for_gradcam = get_top_k_targets(model, img_tensor)\n",
    "\n",
    "#                 # 使用标签获取索引和描述\n",
    "#                 index_description = label_to_index_description.get(label)\n",
    "#                 if index_description is None:\n",
    "#                     print(f\"Warning: Label '{label}' not found in the JSON file!\")\n",
    "#                     continue\n",
    "                \n",
    "#                 index_str, description = index_description\n",
    "#                 index = int(index_str)  # 将字符串索引转换为整数\n",
    "\n",
    "#                 dynamic_targets_for_gradcam = [ClassifierOutputTarget(index)]\n",
    "\n",
    "#                 gradcam_result, grayscale_cams = run_grad_cam_on_image(\n",
    "#                     model=model,\n",
    "#                     target_layer=target_layer,\n",
    "#                     targets_for_gradcam=dynamic_targets_for_gradcam,\n",
    "#                     input_tensor=img_tensor,\n",
    "#                     input_image=img,\n",
    "#                     reshape_transform=None\n",
    "#                 )\n",
    "\n",
    "#                 # Extract top predictions\n",
    "#                 logits = model(img_tensor.unsqueeze(0)).logits\n",
    "#                 top_indices = logits[0].argsort(descending=True)[:5].cpu().numpy()\n",
    "#                 predictions = {index: {\"score\": logits[0][index].item(), \"label\": model.config.id2label[index]} for index in top_indices}\n",
    "                \n",
    "#                 # Define individual directory for this image using the original filename\n",
    "#                 img_dir = os.path.join(save_dir, filename.rsplit('.', 1)[0])  # Remove file extension from filename\n",
    "#                 if not os.path.exists(img_dir):\n",
    "#                     os.makedirs(img_dir)\n",
    "                \n",
    "#                 # Define file names\n",
    "#                 img_name = os.path.join(img_dir, \"original.jpg\")\n",
    "#                 gradcam_name = os.path.join(img_dir, \"gradcam.jpg\")\n",
    "#                 grayscale_name = os.path.join(img_dir, \"grayscale.jpg\")\n",
    "#                 grayscale_npy_name = os.path.join(img_dir, \"grayscale.npy\")\n",
    "#                 scores_name = os.path.join(img_dir, \"scores.npy\")\n",
    "#                 info_name = os.path.join(img_dir, \"info.txt\")\n",
    "\n",
    "#                 # Save the images and results\n",
    "#                 img.save(img_name)\n",
    "#                 Image.fromarray(gradcam_result).save(gradcam_name)\n",
    "#                 Image.fromarray((grayscale_cams[0] * 255).astype(np.uint8)).save(grayscale_name)\n",
    "#                 np.save(grayscale_npy_name, grayscale_cams[0])\n",
    "\n",
    "#                 # Save the scores\n",
    "#                 scores = [data[\"score\"] for _, data in predictions.items()]\n",
    "#                 np.save(scores_name, scores)\n",
    "\n",
    "#                 # Save the other info\n",
    "#                 with open(info_name, 'w') as f:\n",
    "#                     for index, data in predictions.items():\n",
    "#                         label = data[\"label\"]\n",
    "#                         score = data[\"score\"]\n",
    "#                         f.write(f\"Class {index} ({label}): {score:.2f}\\n\")\n",
    "#         except RuntimeError as e:\n",
    "#             if \"CUDA out of memory\" in str(e):\n",
    "#                 print(f\"CUDA OutOfMemoryError encountered for file: {filename}\")\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 raise e\n",
    "\n",
    "#         #del img, label, img_name, img_tensor, gradcam_result, grayscale_cams, logits, top_indices, predictions, scores, index_description, index_str, description, index, dynamic_targets_for_gradcam\n",
    "#         torch.cuda.empty_cache()\n",
    "#     # Clean up after processing the batch\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     # Reload the model for the next batch\n",
    "#     model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-small\").to(device)\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# tracker.stop()\n",
    "# emissions_data = tracker.get_emissions()\n",
    "\n",
    "# output_dir = \"/home/workstation/code/XAImethods/pytorch-grad-cam/apple\"\n",
    "# tracker = EmissionsTracker(output_dir=output_dir)\n",
    "\n",
    "# print(emissions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:44:24] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:44:24] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:44:24] Tracking Nvidia GPU via pynvml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:44:24] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:44:24] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:44:26] CPU Model on constant consumption mode: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
      "[codecarbon INFO @ 12:44:26] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:44:26]   Platform system: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 12:44:26]   Python version: 3.10.9\n",
      "[codecarbon INFO @ 12:44:26]   Available RAM : 15.576 GB\n",
      "[codecarbon INFO @ 12:44:26]   CPU count: 16\n",
      "[codecarbon INFO @ 12:44:26]   CPU model: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
      "[codecarbon INFO @ 12:44:26]   GPU count: 1\n",
      "[codecarbon INFO @ 12:44:26]   GPU model: 1 x NVIDIA GeForce RTX 4090\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([3, 480, 640])\n",
      "torch.Size([1, 1536, 15, 20])\n",
      "torch.Size([1, 1536, 15, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 480, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m dynamic_targets_for_gradcam \u001b[39m=\u001b[39m [ClassifierOutputTarget(index)]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39m# print(\"Input tensor shape:\", img_tensor.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# print(\"Calculated width:\", img_tensor.shape[2]//32)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# print(\"Calculated height:\", img_tensor.shape[1]//32)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m gradcam_result, grayscale_cams \u001b[39m=\u001b[39m run_grad_cam_on_image(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     target_layer\u001b[39m=\u001b[39;49mtarget_layer,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     targets_for_gradcam\u001b[39m=\u001b[39;49mdynamic_targets_for_gradcam,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     input_tensor\u001b[39m=\u001b[39;49mimg_tensor,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     input_image\u001b[39m=\u001b[39;49mimg,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     reshape_transform\u001b[39m=\u001b[39;49mreshape_transform\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m logits \u001b[39m=\u001b[39m model(img_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mlogits\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m top_indices \u001b[39m=\u001b[39m logits[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margsort(descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:\u001b[39m5\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mwith\u001b[39;00m method(model\u001b[39m=\u001b[39mHuggingfaceToTensorModelWrapper(model),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m             target_layers\u001b[39m=\u001b[39m[target_layer],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m             reshape_transform\u001b[39m=\u001b[39mreshape_transform) \u001b[39mas\u001b[39;00m cam:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     \u001b[39m# Replicate the tensor for each of the categories we want to create Grad-CAM for:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     repeated_tensor \u001b[39m=\u001b[39m input_tensor[\u001b[39mNone\u001b[39;00m, :]\u001b[39m.\u001b[39mrepeat(\u001b[39mlen\u001b[39m(targets_for_gradcam), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     batch_results \u001b[39m=\u001b[39m cam(input_tensor\u001b[39m=\u001b[39;49mrepeated_tensor,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m                         targets\u001b[39m=\u001b[39;49mtargets_for_gradcam)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     grayscale_cams \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[1;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_input_gradient:\n\u001b[1;32m     71\u001b[0m     input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable(input_tensor,\n\u001b[1;32m     72\u001b[0m                                            requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivations_and_grads(input_tensor)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     target_categories \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/pytorch_grad_cam/activations_and_gradients.py:42\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients \u001b[39m=\u001b[39m []\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb 单元格 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/pytorch-grad-cam/tutorials/microsoft-swin.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:1205\u001b[0m, in \u001b[0;36mSwinForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[39m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1205\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswin(\n\u001b[1;32m   1206\u001b[0m     pixel_values,\n\u001b[1;32m   1207\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1208\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1209\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1210\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1211\u001b[0m )\n\u001b[1;32m   1213\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1215\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:1012\u001b[0m, in \u001b[0;36mSwinModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdepths))\n\u001b[1;32m   1010\u001b[0m embedding_output, input_dimensions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[39m=\u001b[39mbool_masked_pos)\n\u001b[0;32m-> 1012\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1013\u001b[0m     embedding_output,\n\u001b[1;32m   1014\u001b[0m     input_dimensions,\n\u001b[1;32m   1015\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1016\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1017\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1018\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[1;32m   1021\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1022\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:839\u001b[0m, in \u001b[0;36mSwinEncoder.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    836\u001b[0m         create_custom_forward(layer_module), hidden_states, input_dimensions, layer_head_mask\n\u001b[1;32m    837\u001b[0m     )\n\u001b[1;32m    838\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 839\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    840\u001b[0m         hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    843\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    844\u001b[0m hidden_states_before_downsampling \u001b[39m=\u001b[39m layer_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:757\u001b[0m, in \u001b[0;36mSwinStage.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[39mfor\u001b[39;00m i, layer_module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m    755\u001b[0m     layer_head_mask \u001b[39m=\u001b[39m head_mask[i] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    758\u001b[0m         hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n\u001b[1;32m    759\u001b[0m     )\n\u001b[1;32m    761\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    763\u001b[0m hidden_states_before_downsampling \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:688\u001b[0m, in \u001b[0;36mSwinLayer.forward\u001b[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(hidden_states_windows\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 688\u001b[0m attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    689\u001b[0m     hidden_states_windows, attn_mask, head_mask, output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    690\u001b[0m )\n\u001b[1;32m    692\u001b[0m attention_output \u001b[39m=\u001b[39m attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m attention_windows \u001b[39m=\u001b[39m attention_output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, channels)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:563\u001b[0m, in \u001b[0;36mSwinAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    557\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    558\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    562\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 563\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(hidden_states, attention_mask, head_mask, output_attentions)\n\u001b[1;32m    564\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    565\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/transformers/models/swin/modeling_swin.py:495\u001b[0m, in \u001b[0;36mSwinSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    491\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores\u001b[39m.\u001b[39mview(\n\u001b[1;32m    492\u001b[0m         batch_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m mask_shape, mask_shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, dim, dim\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_attention_heads, dim, dim)\n\u001b[1;32m    497\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:44:37] Energy consumed for RAM : 0.000243 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:44:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:37] Energy consumed for all CPUs : 0.001979 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:44:37] 0.002222 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:44] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:44:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:44] Energy consumed for all CPUs : 0.000198 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:44:44] 0.000222 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:52] Energy consumed for RAM : 0.000268 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:44:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:52] Energy consumed for all CPUs : 0.002177 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:44:52] 0.002445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:44:59] Energy consumed for RAM : 0.000049 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:44:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:44:59] Energy consumed for all CPUs : 0.000396 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:44:59] 0.000445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:07] Energy consumed for RAM : 0.000292 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:07] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:07] Energy consumed for all CPUs : 0.002375 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:07] 0.002667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:14] Energy consumed for RAM : 0.000073 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:14] Energy consumed for all CPUs : 0.000594 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:14] 0.000667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:22] Energy consumed for RAM : 0.000316 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:22] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:22] Energy consumed for all CPUs : 0.002573 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:22] 0.002889 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:29] Energy consumed for RAM : 0.000097 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:29] Energy consumed for all CPUs : 0.000792 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:29] 0.000889 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:37] Energy consumed for RAM : 0.000340 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:37] Energy consumed for all CPUs : 0.002771 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:37] 0.003111 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:44] Energy consumed for RAM : 0.000122 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:44] Energy consumed for all CPUs : 0.000990 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:44] 0.001111 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:52] Energy consumed for RAM : 0.000365 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:52] Energy consumed for all CPUs : 0.002969 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:52] 0.003333 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:45:59] Energy consumed for RAM : 0.000146 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:45:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:45:59] Energy consumed for all CPUs : 0.001188 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:45:59] 0.001334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:07] Energy consumed for RAM : 0.000389 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:07] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:07] Energy consumed for all CPUs : 0.003167 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:07] 0.003556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:14] Energy consumed for RAM : 0.000170 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:14] Energy consumed for all CPUs : 0.001385 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:14] 0.001556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:22] Energy consumed for RAM : 0.000413 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:22] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:22] Energy consumed for all CPUs : 0.003364 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:22] 0.003778 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:29] Energy consumed for RAM : 0.000195 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:29] Energy consumed for all CPUs : 0.001583 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:29] 0.001778 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:37] Energy consumed for RAM : 0.000438 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:37] Energy consumed for all CPUs : 0.003562 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:37] 0.004000 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:44] Energy consumed for RAM : 0.000219 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:44] Energy consumed for all CPUs : 0.001781 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:44] 0.002000 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:52] Energy consumed for RAM : 0.000462 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:52] Energy consumed for all CPUs : 0.003760 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:52] 0.004222 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:46:59] Energy consumed for RAM : 0.000243 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:46:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:46:59] Energy consumed for all CPUs : 0.001979 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:46:59] 0.002222 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:07] Energy consumed for RAM : 0.000486 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:07] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:07] Energy consumed for all CPUs : 0.003958 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:07] 0.004445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:14] Energy consumed for RAM : 0.000268 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:14] Energy consumed for all CPUs : 0.002177 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:14] 0.002445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:22] Energy consumed for RAM : 0.000511 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:22] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:22] Energy consumed for all CPUs : 0.004156 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:22] 0.004667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:29] Energy consumed for RAM : 0.000292 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:29] Energy consumed for all CPUs : 0.002375 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:29] 0.002667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:37] Energy consumed for RAM : 0.000535 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:37] Energy consumed for all CPUs : 0.004354 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:37] 0.004889 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:44] Energy consumed for RAM : 0.000316 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:44] Energy consumed for all CPUs : 0.002573 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:44] 0.002889 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:52] Energy consumed for RAM : 0.000559 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:52] Energy consumed for all CPUs : 0.004552 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:52] 0.005111 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:47:59] Energy consumed for RAM : 0.000340 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:47:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:47:59] Energy consumed for all CPUs : 0.002771 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:47:59] 0.003111 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:07] Energy consumed for RAM : 0.000584 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:07] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:07] Energy consumed for all CPUs : 0.004750 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:07] 0.005334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:14] Energy consumed for RAM : 0.000365 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:14] Energy consumed for all CPUs : 0.002969 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:14] 0.003334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:22] Energy consumed for RAM : 0.000608 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:22] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:22] Energy consumed for all CPUs : 0.004948 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:22] 0.005556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:29] Energy consumed for RAM : 0.000389 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:29] Energy consumed for all CPUs : 0.003167 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:29] 0.003556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:37] Energy consumed for RAM : 0.000632 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:37] Energy consumed for all CPUs : 0.005146 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:37] 0.005778 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:44] Energy consumed for RAM : 0.000413 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:44] Energy consumed for all CPUs : 0.003365 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:44] 0.003778 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:52] Energy consumed for RAM : 0.000657 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:52] Energy consumed for all CPUs : 0.005344 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:52] 0.006000 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:48:59] Energy consumed for RAM : 0.000438 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:48:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:48:59] Energy consumed for all CPUs : 0.003562 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:48:59] 0.004000 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:07] Energy consumed for RAM : 0.000681 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:07] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:07] Energy consumed for all CPUs : 0.005541 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:07] 0.006222 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:14] Energy consumed for RAM : 0.000462 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:14] Energy consumed for all CPUs : 0.003760 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:14] 0.004222 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:22] Energy consumed for RAM : 0.000705 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:22] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:22] Energy consumed for all CPUs : 0.005739 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:22] 0.006445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:29] Energy consumed for RAM : 0.000486 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:29] Energy consumed for all CPUs : 0.003958 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:29] 0.004445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:37] Energy consumed for RAM : 0.000730 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:37] Energy consumed for all CPUs : 0.005937 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:37] 0.006667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:44] Energy consumed for RAM : 0.000511 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:44] Energy consumed for all CPUs : 0.004156 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:44] 0.004667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:52] Energy consumed for RAM : 0.000754 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:52] Energy consumed for all CPUs : 0.006135 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:52] 0.006889 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:49:59] Energy consumed for RAM : 0.000535 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:49:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:49:59] Energy consumed for all CPUs : 0.004354 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:49:59] 0.004889 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:07] Energy consumed for RAM : 0.000778 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:07] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:07] Energy consumed for all CPUs : 0.006333 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:07] 0.007111 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:14] Energy consumed for RAM : 0.000559 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:14] Energy consumed for all CPUs : 0.004552 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:14] 0.005111 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:22] Energy consumed for RAM : 0.000803 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:22] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:22] Energy consumed for all CPUs : 0.006531 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:22] 0.007334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:29] Energy consumed for RAM : 0.000584 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:29] Energy consumed for all CPUs : 0.004750 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:29] 0.005334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:37] Energy consumed for RAM : 0.000827 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:37] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:37] Energy consumed for all CPUs : 0.006729 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:37] 0.007556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:44] Energy consumed for RAM : 0.000608 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:44] Energy consumed for all CPUs : 0.004948 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:44] 0.005556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:52] Energy consumed for RAM : 0.000851 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:50:52] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:50:52] Energy consumed for all CPUs : 0.006927 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:50:52] 0.007778 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:50:59] Energy consumed for RAM : 0.000632 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:04] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:04] Energy consumed for all CPUs : 0.005209 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:04] 0.005842 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:09] Energy consumed for RAM : 0.000877 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:13] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:14] Energy consumed for RAM : 0.000650 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:16] Energy consumed for all CPUs : 0.007237 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:16] 0.008114 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:16] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:16] Energy consumed for all CPUs : 0.005369 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:16] 0.006018 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:23] Energy consumed for RAM : 0.000889 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:23] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:23] Energy consumed for all CPUs : 0.007335 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:23] 0.008224 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:29] Energy consumed for RAM : 0.000671 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:29] Energy consumed for all CPUs : 0.005547 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:29] 0.006218 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:38] Energy consumed for RAM : 0.000913 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:38] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:38] Energy consumed for all CPUs : 0.007532 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:38] 0.008445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:44] Energy consumed for RAM : 0.000696 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:44] Energy consumed for all CPUs : 0.005745 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:44] 0.006440 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:53] Energy consumed for RAM : 0.000937 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:53] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:53] Energy consumed for all CPUs : 0.007730 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:53] 0.008667 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:51:59] Energy consumed for RAM : 0.000720 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:51:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:51:59] Energy consumed for all CPUs : 0.005943 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:51:59] 0.006663 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:08] Energy consumed for RAM : 0.000962 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:08] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:08] Energy consumed for all CPUs : 0.007928 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:08] 0.008890 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:14] Energy consumed for RAM : 0.000744 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:14] Energy consumed for all CPUs : 0.006140 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:14] 0.006885 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:23] Energy consumed for RAM : 0.000986 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:23] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:23] Energy consumed for all CPUs : 0.008126 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:23] 0.009112 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:29] Energy consumed for RAM : 0.000769 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:29] Energy consumed for all CPUs : 0.006338 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:29] 0.007107 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:38] Energy consumed for RAM : 0.001010 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:38] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:38] Energy consumed for all CPUs : 0.008324 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:38] 0.009334 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:44] Energy consumed for RAM : 0.000793 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:44] Energy consumed for all CPUs : 0.006536 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:44] 0.007329 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:53] Energy consumed for RAM : 0.001035 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:53] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:53] Energy consumed for all CPUs : 0.008522 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:53] 0.009556 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:52:59] Energy consumed for RAM : 0.000817 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:52:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:52:59] Energy consumed for all CPUs : 0.006734 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:52:59] 0.007552 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:08] Energy consumed for RAM : 0.001059 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:08] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:08] Energy consumed for all CPUs : 0.008720 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:08] 0.009779 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:14] Energy consumed for RAM : 0.000842 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:14] Energy consumed for all CPUs : 0.006932 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:14] 0.007774 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:23] Energy consumed for RAM : 0.001083 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:23] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:23] Energy consumed for all CPUs : 0.008917 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:23] 0.010001 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:29] Energy consumed for RAM : 0.000866 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:29] Energy consumed for all CPUs : 0.007130 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:29] 0.007996 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:38] Energy consumed for RAM : 0.001108 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:38] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:38] Energy consumed for all CPUs : 0.009115 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:38] 0.010223 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:44] Energy consumed for RAM : 0.000890 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:44] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:44] Energy consumed for all CPUs : 0.007328 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:44] 0.008218 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:53] Energy consumed for RAM : 0.001132 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:53] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:53] Energy consumed for all CPUs : 0.009313 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:53] 0.010445 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:53:59] Energy consumed for RAM : 0.000915 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:53:59] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:53:59] Energy consumed for all CPUs : 0.007526 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:53:59] 0.008441 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:54:08] Energy consumed for RAM : 0.001156 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:54:08] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:54:08] Energy consumed for all CPUs : 0.009511 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:54:08] 0.010668 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:54:14] Energy consumed for RAM : 0.000939 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:54:14] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:54:14] Energy consumed for all CPUs : 0.007724 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:54:14] 0.008663 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:54:23] Energy consumed for RAM : 0.001181 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:54:23] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:54:23] Energy consumed for all CPUs : 0.009709 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:54:23] 0.010890 kWh of electricity used since the begining.\n",
      "[codecarbon INFO @ 12:54:29] Energy consumed for RAM : 0.000963 kWh. RAM Power : 5.841164588928223 W\n",
      "[codecarbon INFO @ 12:54:29] Energy consumed for all GPUs : 0.000000 kWh. All GPUs Power : 0.0 W\n",
      "[codecarbon INFO @ 12:54:29] Energy consumed for all CPUs : 0.007922 kWh. All CPUs Power : 47.5 W\n",
      "[codecarbon INFO @ 12:54:29] 0.008885 kWh of electricity used since the begining.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from transformers import SwinForImageClassification\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import datetime\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "###################\n",
    "\n",
    "def swinT_reshape_transform_huggingface(tensor, width, height):\n",
    "    result = tensor.reshape(tensor.size(0),\n",
    "                            height,\n",
    "                            width,\n",
    "                            tensor.size(2))\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    print(result.shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"microsoft/swin\"\n",
    "\n",
    "# Initialize tracking variables\n",
    "start_time = datetime.datetime.now()\n",
    "emissions_reports = []\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "\n",
    "def ensure_rgb(img):\n",
    "    if img.mode != 'RGB':\n",
    "        return img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "def is_valid_image_file(filepath):\n",
    "    \"\"\"Check if the file is a valid image file.\"\"\"\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            img.verify()  # verify that it is a valid image\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "num_batches = len(dataset) // BATCH_SIZE + (1 if len(dataset) % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "save_dir = f\"{current_dir}/results/{model_name}/{cam_algorithm_name}\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "for batch_num in tqdm(range(num_batches)):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min((batch_num + 1) * BATCH_SIZE, len(dataset))\n",
    "\n",
    "    model = SwinForImageClassification.from_pretrained(\"microsoft/swin-large-patch4-window12-384-in22k\").to(device)\n",
    "    target_layer = model.swin.layernorm\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        img, label, filename = dataset[idx]\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            img = ensure_rgb(img)\n",
    "            resize_transform = transforms.Resize((480, 640))\n",
    "            img = resize_transform(img)\n",
    "            img_tensor = transform(img).to(device)\n",
    "            print(img_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "            #img_tensor = transform(img).to(device)\n",
    "            # print(\"Input tensor shape:\", img_tensor.shape)\n",
    "            # print(\"Calculated width:\", img_tensor.shape[2]//32)\n",
    "            # print(\"Calculated height:\", img_tensor.shape[1]//32)\n",
    "            reshape_transform = partial(swinT_reshape_transform_huggingface,\n",
    "                                        width=img_tensor.shape[2]//32,\n",
    "                                        height=img_tensor.shape[1]//32)\n",
    "            index_description = label_to_index_description.get(label)\n",
    "            if index_description is None:\n",
    "                print(f\"Warning: Label '{label}' not found in the JSON file!\")\n",
    "                continue\n",
    "\n",
    "            index_str, description = index_description\n",
    "            index = int(index_str)\n",
    "            dynamic_targets_for_gradcam = [ClassifierOutputTarget(index)]\n",
    "\n",
    "            \n",
    "\n",
    "            # print(\"Input tensor shape:\", img_tensor.shape)\n",
    "            # print(\"Calculated width:\", img_tensor.shape[2]//32)\n",
    "            # print(\"Calculated height:\", img_tensor.shape[1]//32)\n",
    "\n",
    "\n",
    "            gradcam_result, grayscale_cams = run_grad_cam_on_image(\n",
    "                model=model,\n",
    "                target_layer=target_layer,\n",
    "                targets_for_gradcam=dynamic_targets_for_gradcam,\n",
    "                input_tensor=img_tensor,\n",
    "                input_image=img,\n",
    "                reshape_transform=reshape_transform\n",
    "            )\n",
    "\n",
    "            logits = model(img_tensor.unsqueeze(0)).logits\n",
    "            top_indices = logits[0].argsort(descending=True)[:5].cpu().numpy()\n",
    "            predictions = {index: {\"score\": logits[0][index].item(), \"label\": model.config.id2label[index]} for index in top_indices}\n",
    "            \n",
    "            img_dir = os.path.join(save_dir, filename.rsplit('.', 1)[0])\n",
    "            if not os.path.exists(img_dir):\n",
    "                os.makedirs(img_dir)\n",
    "\n",
    "            img_name = os.path.join(img_dir, \"original.jpg\")\n",
    "            gradcam_name = os.path.join(img_dir, \"gradcam.jpg\")\n",
    "            grayscale_name = os.path.join(img_dir, \"grayscale.jpg\")\n",
    "            grayscale_npy_name = os.path.join(img_dir, \"grayscale.npy\")\n",
    "            scores_name = os.path.join(img_dir, \"scores.npy\")\n",
    "            info_name = os.path.join(img_dir, \"info.txt\")\n",
    "\n",
    "            img.save(img_name)\n",
    "            Image.fromarray(gradcam_result).save(gradcam_name)\n",
    "            Image.fromarray((grayscale_cams[0] * 255).astype(np.uint8)).save(grayscale_name)\n",
    "            np.save(grayscale_npy_name, grayscale_cams[0])\n",
    "\n",
    "            scores = [data[\"score\"] for _, data in predictions.items()]\n",
    "            np.save(scores_name, scores)\n",
    "\n",
    "            with open(info_name, 'w') as f:\n",
    "                for index, data in predictions.items():\n",
    "                    label = data[\"label\"]\n",
    "                    score = data[\"score\"]\n",
    "                    f.write(f\"Class {index} ({label}): {score:.2f}\\n\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"CUDA OutOfMemoryError encountered for file: {filename}\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    # del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# After processing\n",
    "tracker.stop()\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Get emissions data\n",
    "emissions_data = tracker.get_emissions()\n",
    "\n",
    "# Save emissions report\n",
    "report_filename = os.path.join(save_dir, \"emissions_report.txt\")\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(emissions_data, f, indent=4)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Processing started at: {start_time}\")\n",
    "print(f\"Processing ended at: {end_time}\")\n",
    "print(f\"Duration: {end_time - start_time}\")\n",
    "print(f\"Emissions report saved to: {report_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
